# A15 Real-Time Disaster Response Summarization using LLM with HPC

## Team Members
- **Harikrishna S** - CB.AI.U4AID23011  
- **Deepak Kumar** - CB.AI.U4AID23012  
- **Somavaram J Rishika** - CB.AI.U4AID23038  

## Overview
A high-performance computing (HPC)-enabled, AI-powered system designed to:
- Ingest real-time disaster data from multiple APIs in parallel  
- Normalize and store events in MongoDB with fast query indexing  
- Fine-tune LLM models for disaster classification & summarization  
- Deploy a Retrieval-Augmented Generation (RAG) pipeline for context-aware insights  
- Scale seamlessly using Docker & cloud-native architectures  

## Tech Stack
- **Languages:** Python, Bash  
- **Frameworks:** asyncio, aiohttp, Pydantic, HuggingFace Transformers  
- **Database:** MongoDB (with indexing)  
- **Parallel Computing:** MPI, Multiprocessing, AsyncIO  
- **Containerization:** Docker, Docker Compose  
- **Cloud:** AWS ECS/EKS, GCP Cloud Run, Azure Container Apps  

## Key Features
- âš¡ **Parallel API Ingestion** â†’ Low-latency disaster data collection  
- ğŸ§¹ **Data Normalization & Validation** â†’ Unified schema with Pydantic  
- ğŸ—„ **MongoDB Storage** â†’ Indexed for high-speed retrieval  
- ğŸ¤– **LLM-based Summarization** â†’ Context-aware disaster reports  
- ğŸ” **RAG Pipeline** â†’ Combines retrieval & generative AI for real-time insights  
- ğŸ³ **Dockerized Deployment** â†’ Cloud-ready with CI/CD integration  

## Workflow
API Ingestion (AsyncIO)
â†’
Data Normalization & Validation (Pydantic)
â†’
MongoDB Storage with Indexing
â†’
Model Fine-Tuning & Preprocessing (Multiprocessing + HPC)
â†’
RAG Pipeline (Retrieval + LLM Summarization)
â†’
Docker + Cloud Deployment
